#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Main script to generate all report formats (Markdown, HTML, Word, etc.)
by orchestrating report_composer and report_renderers.
"""

import os
import logging
import pandas as pd
from typing import List, Dict, Any
import subprocess
import json
import glob

try:
    from scripts.report_composer import compose_report_elements
    from scripts.report_renderers import render_elements_to_markdown, render_elements_to_html, render_elements_to_word
    from scripts.report_utils import create_report_directories, extract_tables_from_markdown_to_excel
except ImportError:
    # Fallback for potential path issues or direct execution
    from report_composer import compose_report_elements
    from report_renderers import render_elements_to_markdown, render_elements_to_html, render_elements_to_word
    from report_utils import create_report_directories, extract_tables_from_markdown_to_excel

logger = logging.getLogger(__name__)

def generate_all_reports(
    data: pd.DataFrame, # Added: The main DataFrame, useful for general stats in intro
    analysis_results: Dict[str, Any],
    figures_list: List[Dict[str, str]], # List of dicts, each with 'path' and 'caption'
    config: Dict[str, Any],
    data_cleaning_summary_md: str = "" # Added: Markdown string for data cleaning
) -> List[str]:
    """
    Generates all configured report outputs.

    Args:
        data: The main pandas DataFrame after processing.
        analysis_results: A dictionary containing all data and statistical results.
        figures_list: A list of dictionaries, where each dictionary contains 'path' 
                      (relative to project root, e.g., 'output/figures/some_plot.png') 
                      and 'caption' for a figure.
        config: A dictionary containing configuration options, including:
                - report_title (str): The main title for the report.
                - base_output_dir (str): The base directory for all outputs (e.g., "output").
                - reports_subdir (str): Subdirectory for reports (e.g., "reports").
                - tables_subdir (str): Subdirectory for tables (e.g., "tables").
                - markdown_filename (str): Filename for the Markdown report.
                - html_filename (str): Filename for the HTML report.
                - word_filename (str): Filename for the Word report.
                - excel_filename (str): Filename for the Excel table summary.
        data_cleaning_summary_md: Markdown string detailing data cleaning steps and results.
    """
    generated_paths = [] # To store paths of generated reports
    logger.info("============================== å¼€å§‹ç”Ÿæˆæ‰€æœ‰æŠ¥å‘Š ==============================")

    base_output_dir = config.get("base_output_dir", "output")
    reports_subdir = config.get("reports_subdir", "reports")
    tables_subdir = config.get("tables_subdir", "tables")

    report_output_dir = os.path.join(base_output_dir, reports_subdir)
    table_output_dir = os.path.join(base_output_dir, tables_subdir)

    # Create directories if they don't exist
    create_report_directories(base_output_dir=base_output_dir)

    logger.info("ğŸ“ å¼€å§‹ç”Ÿæˆç»“æ„åŒ–æŠ¥å‘Šå…ƒç´ ...")
    # Ensure figures_list paths are adjusted if necessary (e.g. making them relative to a figures dir)
    # For now, assume paths in figures_list are suitable for embedding or direct use.
    # The composer might expect paths relative to where the final MD/HTML is, or absolute.
    # Let's assume 'output_figures_dir' is part of config for composer.
    
    # Update config for composer if needed, e.g., with image directory information
    composer_config = config.copy() # Avoid modifying original config dict directly if not intended
    # composer_config['output_figures_dir'] = os.path.join(base_output_dir, config.get('figures_subdir', 'figures'))


    report_elements = compose_report_elements(
        data=data,
        analysis_results=analysis_results, 
        figures_list=figures_list, 
        config=composer_config,
        data_cleaning_summary_md=data_cleaning_summary_md # Pass it here
    )
    logger.info(f"ğŸ“„ ç»“æ„åŒ–æŠ¥å‘Šå…ƒç´ ç”Ÿæˆå®Œæ¯•ï¼Œå…± {len(report_elements)} ä¸ªå…ƒç´ ã€‚")

    # --- Generate Markdown Report ---
    md_filename = config.get("markdown_filename", "PWVæ•°æ®åˆ†æç»¼åˆæŠ¥å‘Š.md")
    md_filepath = os.path.join(report_output_dir, md_filename)
    logger.info(f"ğŸ”„ æ­£åœ¨ç”ŸæˆMarkdownæŠ¥å‘Š: {md_filepath}...")
    try:
        render_elements_to_markdown(report_elements, md_filepath, config=config)
        logger.info(f"âœ… MarkdownæŠ¥å‘Šå·²ä¿å­˜è‡³: {md_filepath}")
        generated_paths.append(md_filepath)
    except Exception as e:
        logger.error(f"âŒ ç”ŸæˆMarkdownæŠ¥å‘Šå¤±è´¥: {e}", exc_info=True)
        # Continue to other formats if possible

    # --- Extract Tables from Markdown to Excel ---
    excel_filename = config.get("excel_filename", "PWVæ•°æ®åˆ†ææŠ¥å‘Š_è¡¨æ ¼æ±‡æ€».xlsx")
    excel_filepath = os.path.join(table_output_dir, excel_filename)
    logger.info(f"ğŸ”„ ä» {md_filepath} æå–è¡¨æ ¼æ•°æ®åˆ°Excel: {excel_filepath}...")
    try:
        # This utility would need to parse the generated markdown file
        extract_tables_from_markdown_to_excel(md_filepath, excel_filepath)
        logger.info(f"âœ… æˆåŠŸæå–è¡¨æ ¼åˆ°: {excel_filepath}")
        generated_paths.append(excel_filepath)
    except Exception as e:
        logger.error(f"âŒ ä»Markdownæå–è¡¨æ ¼åˆ°Excelå¤±è´¥: {e}", exc_info=True)

    # --- Generate HTML Report ---
    html_filename = config.get("html_filename", "PWVæ•°æ®åˆ†æç»¼åˆæŠ¥å‘Š.html")
    html_filepath = os.path.join(report_output_dir, html_filename)
    logger.info(f"ğŸ”„ æ­£åœ¨å°†ç»“æ„åŒ–å…ƒç´ è½¬æ¢ä¸ºHTMLæ–‡æ¡£: {html_filepath}...")
    try:
        render_elements_to_html(report_elements, html_filepath, config=config)
        logger.info(f"âœ… HTMLæŠ¥å‘Šå·²ä¿å­˜è‡³: {html_filepath}")
        generated_paths.append(html_filepath)
    except Exception as e:
        logger.error(f"âŒ ç”ŸæˆHTMLæŠ¥å‘Šå¤±è´¥: {e}", exc_info=True)

    # --- Generate Word Report ---
    word_filename = config.get("word_filename", "PWVæ•°æ®åˆ†æç»¼åˆæŠ¥å‘Š.docx")
    word_filepath = os.path.join(report_output_dir, word_filename)
    logger.info(f"ğŸ”„ æ­£åœ¨å°†ç»“æ„åŒ–å…ƒç´ è½¬æ¢ä¸ºWordæ–‡æ¡£: {word_filepath}...")
    try:
        render_elements_to_word(report_elements, word_filepath, config=config)
        logger.info(f"âœ… WordæŠ¥å‘Šå·²ä¿å­˜è‡³: {word_filepath}")
        generated_paths.append(word_filepath)
    except Exception as e:
        logger.error(f"âŒ ç”ŸæˆWordæŠ¥å‘Šå¤±è´¥: {e}", exc_info=True)

    logger.info("============================== æ‰€æœ‰æŠ¥å‘Šç”Ÿæˆå®Œæ¯• ==============================")
    for path in generated_paths:
        logger.info(f"  -> {path}")
    
    return generated_paths # Return the list of paths


if __name__ == '__main__':
    # This is a basic test execution.
    # In a real scenario, analysis_results and figures_list would be populated by the analysis pipeline.
    
    print("è¿è¡Œ report_generator.py ä½œä¸ºç‹¬ç«‹è„šæœ¬è¿›è¡Œæµ‹è¯•...")

    # Setup basic logging for the test
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

    # Initialize analysis_results and figures_list
    mock_analysis_results = {}
    mock_figures_list = []

    # --- Run Data Missing and Cleaning Analysis ---
    data_cleaning_script_path = os.path.join("scripts", "temp_analysis", "data_missing_and_cleaning_analysis.py")
    data_cleaning_md_report_path = os.path.join("scripts", "temp_analysis", "data_missing_and_cleaning_analysis.md")
    data_prep_summary_path = os.path.join("output", "data", "data_preparation_summary.json")

    logger.info(f"æ‰§è¡Œæ•°æ®æ¸…æ´—è„šæœ¬: {data_cleaning_script_path}...")
    try:
        # Ensure the script is executable or run with python interpreter
        process = subprocess.run(['python3', data_cleaning_script_path], capture_output=True, text=True, check=True)
        logger.info("æ•°æ®æ¸…æ´—è„šæœ¬æ‰§è¡ŒæˆåŠŸã€‚")
        # print(process.stdout) # Optional: print script output
        if process.stderr:
            logger.warning(f"æ•°æ®æ¸…æ´—è„šæœ¬æ ‡å‡†é”™è¯¯è¾“å‡º:\n{process.stderr}")

        # Load the generated Markdown content for data cleaning details
        if os.path.exists(data_cleaning_md_report_path):
            with open(data_cleaning_md_report_path, 'r', encoding='utf-8') as f:
                mock_analysis_results['data_missing_and_cleaning_details_md'] = f.read()
            logger.info(f"å·²åŠ è½½æ•°æ®æ¸…æ´—è¯¦æƒ…æŠ¥å‘Š: {data_cleaning_md_report_path}")
        else:
            logger.warning(f"æ•°æ®æ¸…æ´—è¯¦æƒ…æŠ¥å‘Šæœªæ‰¾åˆ°: {data_cleaning_md_report_path}")
            mock_analysis_results['data_missing_and_cleaning_details_md'] = "[æ•°æ®æ¸…æ´—ä¸ç¼ºå¤±å€¼åˆ†æçš„è¯¦ç»†æŠ¥å‘Šæœªç”Ÿæˆæˆ–æœªæ‰¾åˆ°]"

        # Load data preparation summary
        if os.path.exists(data_prep_summary_path):
            with open(data_prep_summary_path, 'r', encoding='utf-8') as f:
                mock_analysis_results['data_preparation_summary'] = json.load(f)
            logger.info(f"å·²åŠ è½½æ•°æ®å‡†å¤‡æ‘˜è¦: {data_prep_summary_path}")
        else:
            logger.warning(f"æ•°æ®å‡†å¤‡æ‘˜è¦ JSON æ–‡ä»¶æœªæ‰¾åˆ°: {data_prep_summary_path}")
            mock_analysis_results['data_preparation_summary'] = {} # Provide empty dict as fallback

    except subprocess.CalledProcessError as e:
        logger.error(f"æ‰§è¡Œæ•°æ®æ¸…æ´—è„šæœ¬ {data_cleaning_script_path} å¤±è´¥. è¿”å›ç : {e.returncode}")
        logger.error(f"æ ‡å‡†è¾“å‡º:\n{e.stdout}")
        logger.error(f"æ ‡å‡†é”™è¯¯:\n{e.stderr}")
        mock_analysis_results['data_missing_and_cleaning_details_md'] = "[æ•°æ®æ¸…æ´—ä¸ç¼ºå¤±å€¼åˆ†æè„šæœ¬æ‰§è¡Œå¤±è´¥ï¼Œè¯¦æƒ…è¯·æŸ¥çœ‹æ—¥å¿—]"
        mock_analysis_results['data_preparation_summary'] = {}
    except FileNotFoundError:
        logger.error(f"æ•°æ®æ¸…æ´—è„šæœ¬ {data_cleaning_script_path} æœªæ‰¾åˆ°ã€‚è¯·ç¡®ä¿è·¯å¾„æ­£ç¡®ã€‚")
        mock_analysis_results['data_missing_and_cleaning_details_md'] = "[æ•°æ®æ¸…æ´—ä¸ç¼ºå¤±å€¼åˆ†æè„šæœ¬æœªæ‰¾åˆ°]"
        mock_analysis_results['data_preparation_summary'] = {}
    except Exception as e:
        logger.error(f"åŠ è½½æ•°æ®æ¸…æ´—è„šæœ¬è¾“å‡ºæ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}", exc_info=True)
        mock_analysis_results['data_missing_and_cleaning_details_md'] = "[åŠ è½½æ•°æ®æ¸…æ´—æŠ¥å‘Šæ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯]"
        mock_analysis_results['data_preparation_summary'] = {}

    # --- Run Subgroup Analysis ---
    subgroup_script_path = os.path.join("scripts", "temp_analysis", "subgroup_analysis.py")
    subgroup_summary_csv_path = os.path.join("output", "subgroup_analysis", "subgroup_analysis_summary.csv")
    subgroup_figures_base_dir = os.path.join("output", "subgroup_analysis")

    logger.info(f"æ‰§è¡Œäºšç»„åˆ†æè„šæœ¬: {subgroup_script_path}...")
    try:
        process = subprocess.run(['python3', subgroup_script_path], capture_output=True, text=True, check=True)
        logger.info("äºšç»„åˆ†æè„šæœ¬æ‰§è¡ŒæˆåŠŸã€‚")
        if process.stderr:
            logger.warning(f"äºšç»„åˆ†æè„šæœ¬æ ‡å‡†é”™è¯¯è¾“å‡º:\n{process.stderr}")

        # Load subgroup analysis summary CSV
        if os.path.exists(subgroup_summary_csv_path):
            mock_analysis_results['subgroup_analysis_summary'] = pd.read_csv(subgroup_summary_csv_path)
            logger.info(f"å·²åŠ è½½äºšç»„åˆ†ææ‘˜è¦: {subgroup_summary_csv_path}")
        else:
            logger.warning(f"äºšç»„åˆ†ææ‘˜è¦CSVæœªæ‰¾åˆ°: {subgroup_summary_csv_path}")
            # mock_analysis_results['subgroup_analysis_summary'] = pd.DataFrame() # Fallback

        # Scan for subgroup analysis figures and add them to mock_figures_list
        figure_patterns = [
            os.path.join(subgroup_figures_base_dir, "cfPWV_é€Ÿåº¦m_s", "**", "boxplot_*.png"),
            os.path.join(subgroup_figures_base_dir, "baPWV_å¹³å‡é€Ÿåº¦m_s", "**", "boxplot_*.png"),
            os.path.join(subgroup_figures_base_dir, "ABI_Overall", "**", "boxplot_*.png") # Add other targets if any
        ]
        
        found_subgroup_figures = 0
        for pattern in figure_patterns:
            for fig_path in glob.glob(pattern, recursive=True):
                # Construct a somewhat descriptive caption from the path
                # Path: output/subgroup_analysis/TARGET/GROUP/boxplot_TARGET_by_GROUP.png
                parts = fig_path.split(os.sep)
                try:
                    group_name = parts[-2]
                    target_variable_full = parts[-3]
                    # Clean up target variable name for caption, e.g., "cfPWV_é€Ÿåº¦m_s" -> "cfPWV é€Ÿåº¦m/s"
                    target_variable_cleaned = target_variable_full.replace("_", " ") 
                    filename = os.path.basename(fig_path)
                    caption = f"{target_variable_cleaned} by {group_name} - {filename}"
                except IndexError:
                    caption = os.path.basename(fig_path) # Fallback caption
                
                # Ensure path is relative to project root for consistency if needed, though glob gives full/relative based on pattern
                # For now, assuming the paths from glob are what composer expects or can handle.
                # Convert to relative path from project root if fig_path is absolute for portability
                relative_fig_path = os.path.relpath(fig_path, os.getcwd()) # getcwd() is workspace root here
                mock_figures_list.append({'path': relative_fig_path, 'caption': caption})
                found_subgroup_figures += 1
        logger.info(f"æ‰«æåˆ° {found_subgroup_figures} ä¸ªäºšç»„åˆ†æå›¾è¡¨å¹¶å·²æ·»åŠ åˆ° figures_listã€‚")

    except subprocess.CalledProcessError as e:
        logger.error(f"æ‰§è¡Œäºšç»„åˆ†æè„šæœ¬ {subgroup_script_path} å¤±è´¥. è¿”å›ç : {e.returncode}")
        logger.error(f"æ ‡å‡†è¾“å‡º:\n{e.stdout}")
        logger.error(f"æ ‡å‡†é”™è¯¯:\n{e.stderr}")
    except FileNotFoundError:
        logger.error(f"äºšç»„åˆ†æè„šæœ¬ {subgroup_script_path} æœªæ‰¾åˆ°ã€‚è¯·ç¡®ä¿è·¯å¾„æ­£ç¡®ã€‚")
    except Exception as e:
        logger.error(f"åŠ è½½äºšç»„åˆ†æè„šæœ¬è¾“å‡ºæ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}", exc_info=True)

    # --- Run Sample Power Analysis ---
    power_analysis_script_path = os.path.join("scripts", "temp_analysis", "sample_power_analysis.py")
    power_analysis_figures_dir = os.path.join("output", "figures", "other")
    power_analysis_excel_dir = os.path.join("output", "tables") # As defined in sample_power_analysis.py

    logger.info(f"æ‰§è¡Œæ ·æœ¬é‡åŠåŠŸæ•ˆåˆ†æè„šæœ¬: {power_analysis_script_path}...")
    power_excel_path = None
    try:
        process = subprocess.run(['python3', power_analysis_script_path], capture_output=True, text=True, check=True)
        logger.info("æ ·æœ¬é‡åŠåŠŸæ•ˆåˆ†æè„šæœ¬æ‰§è¡ŒæˆåŠŸã€‚")
        # Try to parse the Excel path from the script's stdout
        # Expected output line: "ç»“æœå·²ä¿å­˜åˆ° output/tables/æ ·æœ¬é‡åˆ†æç»“æœ_YYYYMMDD_HHMMSS.xlsx"
        for line in process.stdout.splitlines():
            if "ç»“æœå·²ä¿å­˜åˆ°" in line and ".xlsx" in line:
                power_excel_path = line.split("ç»“æœå·²ä¿å­˜åˆ°")[-1].strip()
                logger.info(f"ä»è„šæœ¬è¾“å‡ºä¸­è§£æå¾—åˆ°Excelæ–‡ä»¶è·¯å¾„: {power_excel_path}")
                break
        
        if process.stderr:
            logger.warning(f"æ ·æœ¬é‡åŠåŠŸæ•ˆåˆ†æè„šæœ¬æ ‡å‡†é”™è¯¯è¾“å‡º:\n{process.stderr}")

        if power_excel_path and os.path.exists(power_excel_path):
            mock_analysis_results['power_analysis_effect_sizes'] = pd.read_excel(power_excel_path, sheet_name='æ•ˆåº”é‡è®¡ç®—ç»“æœ')
            mock_analysis_results['power_analysis_sample_requirements'] = pd.read_excel(power_excel_path, sheet_name='æ ·æœ¬é‡éœ€æ±‚åˆ†æ')
            logger.info(f"å·²åŠ è½½æ ·æœ¬é‡åŠåŠŸæ•ˆåˆ†æExcelæ•°æ®: {power_excel_path}")
            # Add the excel itself to analysis_results for potential direct linking or full table display if needed
            mock_analysis_results['power_analysis_excel_path'] = power_excel_path
        elif power_excel_path: # Path parsed but file not found
            logger.warning(f"æ ·æœ¬é‡åˆ†æExcelæ–‡ä»¶ {power_excel_path} æœªæ‰¾åˆ°ï¼Œå°½ç®¡è·¯å¾„å·²ä»è¾“å‡ºè§£æã€‚")
        else:
            logger.warning("æœªèƒ½ä»æ ·æœ¬é‡åˆ†æè„šæœ¬è¾“å‡ºä¸­è§£æExcelæ–‡ä»¶è·¯å¾„ã€‚ç›¸å…³æ•°æ®å¯èƒ½æ— æ³•åŠ è½½ã€‚")

        # Scan for power analysis figures
        power_figure_patterns = [
            os.path.join(power_analysis_figures_dir, "å¯¹æ¯”æ•ˆåº”é‡çš„æ ·æœ¬é‡éœ€æ±‚_*.png"),
            os.path.join(power_analysis_figures_dir, "*_power_curve.png")
        ]
        found_power_figures = 0
        for pattern in power_figure_patterns:
            for fig_path in glob.glob(pattern):
                # filename_body = os.path.splitext(os.path.basename(fig_path))[0]
                # caption = filename_body.replace("_", " ").capitalize()
                # A more specific caption logic based on filename patterns:
                filename = os.path.basename(fig_path)
                caption = ""
                if "å¯¹æ¯”æ•ˆåº”é‡çš„æ ·æœ¬é‡éœ€æ±‚_" in filename:
                    var_name = filename.replace("å¯¹æ¯”æ•ˆåº”é‡çš„æ ·æœ¬é‡éœ€æ±‚_", "").replace(".png", "")
                    caption = f"æ ·æœ¬é‡éœ€æ±‚å¯¹æ¯” - {var_name.replace('_', ' ').title()}"
                elif "_power_curve.png" in filename:
                    var_name = filename.replace("_power_curve.png", "")
                    caption = f"åŠŸæ•ˆåˆ†ææ›²çº¿ - {var_name.replace('_', ' ').title()}"
                else:
                    caption = os.path.splitext(filename)[0].replace("_", " ").capitalize() # Fallback

                relative_fig_path = os.path.relpath(fig_path, os.getcwd())
                mock_figures_list.append({'path': relative_fig_path, 'caption': caption})
                found_power_figures += 1
        logger.info(f"æ‰«æåˆ° {found_power_figures} ä¸ªæ ·æœ¬é‡åŠåŠŸæ•ˆåˆ†æå›¾è¡¨å¹¶å·²æ·»åŠ åˆ° figures_listã€‚")

    except subprocess.CalledProcessError as e:
        logger.error(f"æ‰§è¡Œæ ·æœ¬é‡åŠåŠŸæ•ˆåˆ†æè„šæœ¬ {power_analysis_script_path} å¤±è´¥. è¿”å›ç : {e.returncode}")
        logger.error(f"æ ‡å‡†è¾“å‡º:\n{e.stdout}")
        logger.error(f"æ ‡å‡†é”™è¯¯:\n{e.stderr}")
    except FileNotFoundError:
        logger.error(f"æ ·æœ¬é‡åŠåŠŸæ•ˆåˆ†æè„šæœ¬ {power_analysis_script_path} æœªæ‰¾åˆ°ã€‚è¯·ç¡®ä¿è·¯å¾„æ­£ç¡®ã€‚")
    except Exception as e:
        logger.error(f"åŠ è½½æ ·æœ¬é‡åŠåŠŸæ•ˆåˆ†æè„šæœ¬è¾“å‡ºæ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}", exc_info=True)

    # --- Run Vascular Age Analysis ---
    vascular_age_script_path = os.path.join("scripts", "temp_analysis", "vascular_age_analysis.py")
    vascular_age_output_base_dir = os.path.join("output", "figures", "other") # As per script's default output

    logger.info(f"æ‰§è¡Œè¡€ç®¡å¹´é¾„åˆ†æè„šæœ¬: {vascular_age_script_path}...")
    try:
        process = subprocess.run(['python3', vascular_age_script_path], capture_output=True, text=True, check=True)
        logger.info("è¡€ç®¡å¹´é¾„åˆ†æè„šæœ¬æ‰§è¡ŒæˆåŠŸã€‚")
        if process.stderr:
            logger.warning(f"è¡€ç®¡å¹´é¾„åˆ†æè„šæœ¬æ ‡å‡†é”™è¯¯è¾“å‡º:\n{process.stderr}")
        # We might need to parse stdout for model performance if not saved to a file.
        # For now, focusing on figures and the Excel file.

        # Load vascular age statistics from Excel
        vascular_stats_excel_path = os.path.join(vascular_age_output_base_dir, "vascular_age_statistics.xlsx")
        if os.path.exists(vascular_stats_excel_path):
            excel_data = pd.read_excel(vascular_stats_excel_path, sheet_name=None) # Read all sheets
            if "å¹´é¾„ç»„ç»Ÿè®¡" in excel_data:
                mock_analysis_results['vascular_age_by_age_group_stats'] = excel_data["å¹´é¾„ç»„ç»Ÿè®¡"]
            if "è¡€ç®¡çŠ¶æ€åˆ†å¸ƒ" in excel_data:
                mock_analysis_results['vascular_age_status_distribution'] = excel_data["è¡€ç®¡çŠ¶æ€åˆ†å¸ƒ"]
            if "æ€§åˆ«åˆ†ç»„ç»Ÿè®¡" in excel_data:
                mock_analysis_results['vascular_age_by_gender_stats'] = excel_data["æ€§åˆ«åˆ†ç»„ç»Ÿè®¡"]
            mock_analysis_results['vascular_age_excel_path'] = vascular_stats_excel_path
            logger.info(f"å·²åŠ è½½è¡€ç®¡å¹´é¾„ç»Ÿè®¡Excelæ•°æ®: {vascular_stats_excel_path}")
        else:
            logger.warning(f"è¡€ç®¡å¹´é¾„ç»Ÿè®¡Excelæ–‡ä»¶ {vascular_stats_excel_path} æœªæ‰¾åˆ°ã€‚")

        # Scan for vascular age figures
        vascular_age_figure_names = [
            "vascular_age_feature_importance.png",
            "vascular_age_vs_real_age_scatter.png",
            "vascular_age_difference_distribution.png",
            "vascular_age_by_age_group.png"
        ]
        found_vascular_age_figures = 0
        for fig_name in vascular_age_figure_names:
            fig_path = os.path.join(vascular_age_output_base_dir, fig_name)
            if os.path.exists(fig_path):
                caption = fig_name.replace(".png", "").replace("_", " ").capitalize()
                relative_fig_path = os.path.relpath(fig_path, os.getcwd())
                mock_figures_list.append({'path': relative_fig_path, 'caption': caption})
                found_vascular_age_figures += 1
            else:
                logger.warning(f"è¡€ç®¡å¹´é¾„åˆ†æå›¾è¡¨ {fig_path} æœªæ‰¾åˆ°ã€‚")
        logger.info(f"æ‰«æåˆ° {found_vascular_age_figures} ä¸ªè¡€ç®¡å¹´é¾„åˆ†æå›¾è¡¨å¹¶å·²æ·»åŠ åˆ° figures_listã€‚")

    except subprocess.CalledProcessError as e:
        logger.error(f"æ‰§è¡Œè¡€ç®¡å¹´é¾„åˆ†æè„šæœ¬ {vascular_age_script_path} å¤±è´¥. è¿”å›ç : {e.returncode}")
        # logger.error(f"æ ‡å‡†è¾“å‡º:\n{e.stdout}") # Potentially very long
        logger.error(f"æ ‡å‡†é”™è¯¯:\n{e.stderr}")
    except FileNotFoundError:
        logger.error(f"è¡€ç®¡å¹´é¾„åˆ†æè„šæœ¬ {vascular_age_script_path} æœªæ‰¾åˆ°ã€‚è¯·ç¡®ä¿è·¯å¾„æ­£ç¡®ã€‚")
    except Exception as e:
        logger.error(f"åŠ è½½è¡€ç®¡å¹´é¾„åˆ†æè„šæœ¬è¾“å‡ºæ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}", exc_info=True)

    # --- Run Detailed Data Cleaning Analysis ---
    detailed_cleaning_script_path = os.path.join("scripts", "temp_analysis", "data_cleaning_analysis.py")
    detailed_cleaning_figures_dir = os.path.join("output", "figures", "other") 
    # Excel files are saved to output/tables by the script

    logger.info(f"æ‰§è¡Œè¯¦ç»†æ•°æ®æ¸…æ´—åˆ†æè„šæœ¬: {detailed_cleaning_script_path}...")
    parsed_detailed_missing_excel = None
    parsed_tabular_report_excel = None
    try:
        process = subprocess.run(['python3', detailed_cleaning_script_path], capture_output=True, text=True, check=True)
        logger.info("è¯¦ç»†æ•°æ®æ¸…æ´—åˆ†æè„šæœ¬æ‰§è¡ŒæˆåŠŸã€‚")
        if process.stderr:
            logger.warning(f"è¯¦ç»†æ•°æ®æ¸…æ´—åˆ†æè„šæœ¬æ ‡å‡†é”™è¯¯è¾“å‡º:\n{process.stderr}")
        
        # Parse stdout for the two Excel file paths
        # Expected: "ç¼ºå¤±å€¼åˆ†æç»“æœ: path/to/æ•°æ®ç¼ºå¤±ä¸æ¸…æ´—åˆ†æ_timestamp.xlsx"
        # Expected: "æ•°æ®æ¸…æ´—æŠ¥å‘Š: path/to/æ•°æ®æ¸…æ´—æŠ¥å‘Š_timestamp.xlsx"
        for line in process.stdout.splitlines():
            if "ç¼ºå¤±å€¼åˆ†æç»“æœ:" in line and ".xlsx" in line:
                parsed_detailed_missing_excel = line.split("ç¼ºå¤±å€¼åˆ†æç»“æœ:")[-1].strip()
                if parsed_detailed_missing_excel == 'æœªç”Ÿæˆ': parsed_detailed_missing_excel = None
            elif "æ•°æ®æ¸…æ´—æŠ¥å‘Š:" in line and ".xlsx" in line:
                parsed_tabular_report_excel = line.split("æ•°æ®æ¸…æ´—æŠ¥å‘Š:")[-1].strip()
                if parsed_tabular_report_excel == 'æœªç”Ÿæˆ': parsed_tabular_report_excel = None
        
        if parsed_detailed_missing_excel and os.path.exists(parsed_detailed_missing_excel):
            mock_analysis_results['detailed_missing_analysis_excel_path'] = parsed_detailed_missing_excel
            logger.info(f"è¯¦ç»†ç¼ºå¤±åˆ†æExcelå·²è®°å½•: {parsed_detailed_missing_excel}")
        else:
            logger.warning(f"è¯¦ç»†ç¼ºå¤±åˆ†æExcelè·¯å¾„ ({parsed_detailed_missing_excel}) æœªæ‰¾åˆ°æˆ–æ— æ³•è§£æã€‚")

        if parsed_tabular_report_excel and os.path.exists(parsed_tabular_report_excel):
            mock_analysis_results['tabular_cleaning_report_excel_path'] = parsed_tabular_report_excel
            logger.info(f"è¡¨æ ¼åŒ–æ¸…æ´—æŠ¥å‘ŠExcelå·²è®°å½•: {parsed_tabular_report_excel}")
        else:
            logger.warning(f"è¡¨æ ¼åŒ–æ¸…æ´—æŠ¥å‘ŠExcelè·¯å¾„ ({parsed_tabular_report_excel}) æœªæ‰¾åˆ°æˆ–æ— æ³•è§£æã€‚")

        # Scan for detailed cleaning figures
        detailed_cleaning_figure_patterns = [
            os.path.join(detailed_cleaning_figures_dir, "raw_data_missing_rate_comparison.png"),
            os.path.join(detailed_cleaning_figures_dir, "cleaning_effect_*.png"),
            os.path.join(detailed_cleaning_figures_dir, "top_missing_variables_heatmap.png")
        ]
        found_detailed_cleaning_figures = 0
        for pattern in detailed_cleaning_figure_patterns:
            for fig_path in glob.glob(pattern):
                filename = os.path.basename(fig_path)
                caption = filename.replace(".png", "").replace("_", " ").replace("detailed cleaning ", "").capitalize()
                relative_fig_path = os.path.relpath(fig_path, os.getcwd())
                mock_figures_list.append({'path': relative_fig_path, 'caption': caption})
                found_detailed_cleaning_figures += 1
        logger.info(f"æ‰«æåˆ° {found_detailed_cleaning_figures} ä¸ªè¯¦ç»†æ¸…æ´—åˆ†æå›¾è¡¨å¹¶å·²æ·»åŠ åˆ° figures_listã€‚")

    except subprocess.CalledProcessError as e:
        logger.error(f"æ‰§è¡Œè¯¦ç»†æ•°æ®æ¸…æ´—åˆ†æè„šæœ¬ {detailed_cleaning_script_path} å¤±è´¥. è¿”å›ç : {e.returncode}")
        logger.error(f"æ ‡å‡†é”™è¯¯:\n{e.stderr}")
    except FileNotFoundError:
        logger.error(f"è¯¦ç»†æ•°æ®æ¸…æ´—åˆ†æè„šæœ¬ {detailed_cleaning_script_path} æœªæ‰¾åˆ°ã€‚è¯·ç¡®ä¿è·¯å¾„æ­£ç¡®ã€‚")
    except Exception as e:
        logger.error(f"åŠ è½½è¯¦ç»†æ•°æ®æ¸…æ´—åˆ†æè„šæœ¬è¾“å‡ºæ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}", exc_info=True)


    # --- Run Temporary PWV Analysis (temp_pwv_analysis.py) ---
    temp_pwv_script_path = os.path.join("scripts", "temp_analysis", "temp_pwv_analysis.py")
    temp_pwv_script_output_dir = os.path.join("scripts", "temp_analysis") # Script saves figures here
    final_temp_pwv_figures_dir = os.path.join("output", "figures", "pwv_internal_comparison")
    os.makedirs(final_temp_pwv_figures_dir, exist_ok=True)

    logger.info(f"æ‰§è¡Œä¸´æ—¶PWVåˆ†æè„šæœ¬: {temp_pwv_script_path}...")
    captured_temp_pwv_stats = {} # Placeholder for parsed stats
    try:
        process = subprocess.run(['python3', temp_pwv_script_path], capture_output=True, text=True, check=True)
        logger.info("ä¸´æ—¶PWVåˆ†æè„šæœ¬æ‰§è¡ŒæˆåŠŸã€‚")
        if process.stderr:
            logger.warning(f"ä¸´æ—¶PWVåˆ†æè„šæœ¬æ ‡å‡†é”™è¯¯è¾“å‡º:\n{process.stderr}")

        # Attempt to parse key stats from stdout (this will be very basic and fragile)
        # Example: Wilcoxon p-value for baPWV R vs L, and Spearman r for cfPWV vs baPWV_avg
        # "Wilcoxonç¬¦å·ç§©æ£€éªŒ: ç»Ÿè®¡é‡=... På€¼=0.0020"
        # "  cfPWV-é€Ÿåº¦m/s vs baPWV-å¹³å‡é€Ÿåº¦m/s: ... Spearman: r=0.750 (p=0.000)"
        for line in process.stdout.splitlines():
            if "Wilcoxonç¬¦å·ç§©æ£€éªŒ:" in line and "På€¼=" in line:
                try:
                    pval_str = line.split("På€¼=")[-1].strip()
                    captured_temp_pwv_stats['bapwv_rl_diff_pval'] = float(pval_str)
                except:
                    logger.warning(f"æ— æ³•è§£æWilcoxon På€¼ä»: {line}")
            if "cfPWV-é€Ÿåº¦m/s vs baPWV-å¹³å‡é€Ÿåº¦m/s:" in process.stdout: # Check if the full line is in stdout
                 # More complex parsing needed here if we are to get the r value specifically for spearman
                 # This is a very simplified check
                 if "Spearman: r=" in process.stdout and "(p=" in process.stdout:
                    try:
                        # This is still too simplistic and likely to fail or grab wrong values
                        # Consider modifying the source script to output JSON for stats.
                        pass # captured_temp_pwv_stats['cfpwv_vs_bapwv_avg_spearman_r'] = ...
                    except:
                        logger.warning("æ— æ³•ä»stdoutè§£æcfPWV vs baPWVå¹³å‡ Spearman r")
        
        if captured_temp_pwv_stats:
            mock_analysis_results['temp_pwv_analysis_stats'] = captured_temp_pwv_stats
            logger.info(f"å·²æ•è·éƒ¨åˆ†ä¸´æ—¶PWVåˆ†æç»Ÿè®¡æ•°æ®: {captured_temp_pwv_stats}")
        else:
            logger.info("æœªèƒ½ä»ä¸´æ—¶PWVåˆ†æè„šæœ¬çš„stdoutä¸­æ•è·å…³é”®ç»Ÿè®¡æ•°æ®ã€‚")

        # Move figures and add to list
        temp_pwv_figure_names = [
            "pwv_distributions_boxplot.png",
            "cfpwv_vs_bapwv_scatter.png"
        ]
        found_temp_pwv_figures = 0
        for fig_name in temp_pwv_figure_names:
            source_fig_path = os.path.join(temp_pwv_script_output_dir, fig_name)
            dest_fig_path = os.path.join(final_temp_pwv_figures_dir, fig_name)
            if os.path.exists(source_fig_path):
                try:
                    import shutil # Import here as it's a standard library
                    shutil.move(source_fig_path, dest_fig_path)
                    caption = fig_name.replace(".png", "").replace("_", " ").capitalize()
                    relative_fig_path = os.path.relpath(dest_fig_path, os.getcwd())
                    mock_figures_list.append({'path': relative_fig_path, 'caption': caption})
                    found_temp_pwv_figures += 1
                    logger.info(f"å·²ç§»åŠ¨å¹¶è®°å½•å›¾è¡¨: {dest_fig_path}")
                except Exception as e:
                    logger.error(f"ç§»åŠ¨å›¾è¡¨ {source_fig_path} åˆ° {dest_fig_path} å¤±è´¥: {e}") 
            else:
                logger.warning(f"ä¸´æ—¶PWVåˆ†æå›¾è¡¨ {source_fig_path} æœªæ‰¾åˆ°ã€‚")
        logger.info(f"å¤„ç†äº† {found_temp_pwv_figures} ä¸ªä¸´æ—¶PWVåˆ†æå›¾è¡¨ã€‚")

    except subprocess.CalledProcessError as e:
        logger.error(f"æ‰§è¡Œä¸´æ—¶PWVåˆ†æè„šæœ¬ {temp_pwv_script_path} å¤±è´¥. è¿”å›ç : {e.returncode}")
        logger.error(f"æ ‡å‡†é”™è¯¯:\n{e.stderr}")
    except FileNotFoundError:
        logger.error(f"ä¸´æ—¶PWVåˆ†æè„šæœ¬ {temp_pwv_script_path} æœªæ‰¾åˆ°ã€‚è¯·ç¡®ä¿è·¯å¾„æ­£ç¡®ã€‚")
    except Exception as e:
        logger.error(f"åŠ è½½ä¸´æ—¶PWVåˆ†æè„šæœ¬è¾“å‡ºæ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}", exc_info=True)


    logger.info(f"æœ€ç»ˆ mock_analysis_results åŒ…å«é”®: {list(mock_analysis_results.keys())}")
    logger.info(f"æœ€ç»ˆ mock_figures_list åŒ…å« {len(mock_figures_list)} ä¸ªå›¾è¡¨.")

    # Dummy data for testing other parts - can be progressively replaced
    mock_analysis_results.update({
        'basic_stats': pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]}),
        'correlation_analysis': pd.DataFrame({'var1': ['A', 'B'], 'var2': ['C', 'D'], 'r': [0.5, -0.3]}),
        'pwv_stats_by_age_group': pd.DataFrame({
            'å¹´é¾„ç»„': ['<40', '40-60', '>60'],
            'PWVå‡å€¼': [1000, 1250, 1500],
            'PWVæ ‡å‡†å·®': [150, 180, 220]
        }),
    })
    # Placeholder for figures_list, will be populated by subgroup analysis next
    mock_figures_list.extend([
        {'path': 'output/figures/distribution/distribution_age.png', 'caption': 'å¹´é¾„åˆ†å¸ƒå›¾ (ç¤ºä¾‹)'},
        {'path': 'output/figures/correlation/correlation_heatmap.png', 'caption': 'ç›¸å…³æ€§çƒ­å›¾ (ç¤ºä¾‹)'}
    ])
    
    # Create dummy figure files for testing if they don't exist
    # to avoid errors during rendering if image embedding is attempted.
    dummy_figure_dir = "output/figures/distribution"
    os.makedirs(dummy_figure_dir, exist_ok=True)
    dummy_corr_dir = "output/figures/correlation"
    os.makedirs(dummy_corr_dir, exist_ok=True)

    if not os.path.exists("output/figures/distribution/distribution_age.png"):
        with open("output/figures/distribution/distribution_age.png", "w") as f:
            f.write("dummy png content") # Not a real image, just for path existence
    if not os.path.exists("output/figures/correlation/correlation_heatmap.png"):
        with open("output/figures/correlation/correlation_heatmap.png", "w") as f:
            f.write("dummy png content")


    mock_config = {
        "report_title": "PWVæ•°æ®åˆ†æç»¼åˆæŠ¥å‘Š (æµ‹è¯•)",
        "base_output_dir": "output",
        "reports_subdir": "reports",
        "tables_subdir": "tables",
        "figures_subdir": "figures", # Used by composer
        "markdown_filename": "Test_PWV_Report.md",
        "html_filename": "Test_PWV_Report.html",
        "word_filename": "Test_PWV_Report.docx",
        "excel_filename": "Test_PWV_Report_Tables.xlsx",
        "image_base_dir_markdown": "output/figures", # For markdown renderer, relative path to MD file
        "image_base_dir_html": "../figures",      # For HTML renderer, relative path to HTML file
                                                  # This might need adjustment based on actual output structure
    }
    
    # Ensure the base output and subdirectories exist for the test
    os.makedirs(os.path.join(mock_config["base_output_dir"], mock_config["reports_subdir"]), exist_ok=True)
    os.makedirs(os.path.join(mock_config["base_output_dir"], mock_config["tables_subdir"]), exist_ok=True)
    
    logger.info("å¼€å§‹æµ‹è¯• generate_all_reports...")
    generate_all_reports(
        data=mock_df, 
        analysis_results=mock_analysis_results, 
        figures_list=mock_figures_list, 
        config=mock_config,
        data_cleaning_summary_md=mock_data_cleaning_md # Pass the mock cleaning MD
    )
    logger.info("æµ‹è¯• generate_all_reports å®Œæˆã€‚è¯·æ£€æŸ¥ 'output' ç›®å½•ä¸­çš„æµ‹è¯•æŠ¥å‘Šæ–‡ä»¶ã€‚") 